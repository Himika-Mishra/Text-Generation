{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufgzgfPmHkt6",
    "outputId": "ef21b659-b4a0-40a2-bd21-25731e5f32db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zARz8XBAL9bf",
    "outputId": "3b0d1b5a-d78b-468d-a3f1-47a849230960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  SciFi.zip\n",
      "  inflating: internet_archive_scifi_v3.txt  \n"
     ]
    }
   ],
   "source": [
    "!cp /content/drive/MyDrive/SciFi.zip /content\n",
    "!unzip SciFi.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1GWlBH8MadY",
    "outputId": "d1cc255d-3369-4b63-f438-e25ae3d7c87c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARCH # All Stories New and Complete Publisher Editor IF is published bi-monthly by Quinn Publishing Company, Inc., Kingston, New York. Volume #, No. #. Copyright # by Quinn Publishing Company, Inc. A\n",
      "['march', 'all', 'stories', 'new', 'and', 'complete', 'publisher', 'editor', 'if', 'is', 'published', 'bimonthly', 'by', 'quinn', 'publishing', 'company', 'inc', 'kingston', 'new', 'york', 'volume', 'no', 'copyright', 'by', 'quinn', 'publishing', 'company', 'inc', 'application', 'for', 'entry', 'as', 'second', 'class', 'matter', 'at', 'post', 'office', 'buffalo', 'new', 'york', 'pending', 'subscription', 'for', 'issues', 'in', 'us', 'and', 'possessions', 'canada', 'for', 'issues', 'elsewhere', 'aiiow', 'four', 'weeks', 'for', 'change', 'of', 'address', 'all', 'stories', 'appearing', 'in', 'this', 'magazine', 'are', 'fiction', 'any', 'similarity', 'to', 'actual', 'persons', 'is', 'coincidental', 'c', 'a', 'fcopy', 'printed', 'ia', 'us', 'a', 'a', 'chat', 'with', 'the', 'editor', 'i', 'science', 'fiction', 'magazine', 'called', 'if', 'the', 'title', 'was', 'selected', 'after', 'much', 'thought', 'because', 'of', 'its', 'brevity', 'and', 'on', 'the', 'theory', 'it', 'is', 'indicative', 'of', 'the', 'field', 'and', 'will', 'be', 'easy', 'to', 'remember', 'the', 'tentative', 'title', 'that', 'just', 'morning', 'and', 'couldnt', 'remember', 'it', 'until', 'wed', 'had', 'a', 'cup', 'of', 'coffee', 'it', 'was', 'summarily', 'discarded', 'a', 'great', 'deal', 'of', 'thought', 'and', 'effort', 'lias', 'gone', 'into', 'the', 'formation', 'of', 'this', 'magazine', 'we', 'have', 'had', 'the', 'aid', 'of', 'several', 'very', 'talented', 'and', 'generous', 'people', 'for', 'which', 'we', 'are', 'most', 'grateful', 'much', 'is', 'due', 'them', 'for', 'their', 'warmhearted', 'assistance', 'and', 'now', 'that', 'the', 'bulk', 'of', 'the', 'formative', 'work', 'is', 'done', 'we', 'will', 'try', 'to', 'maintain', 'if', 'as']\n",
      "Total Tokens: 26306769\n",
      "Unique Tokens: 286166\n",
      "Total Sequences: 100000\n"
     ]
    }
   ],
   "source": [
    "#to process the data files\n",
    "import string \n",
    " \n",
    "# loading the document in the memory\n",
    "def load_doc(filename):\n",
    "\t# opening the file in read only mode\n",
    "\tdocument = open(filename, 'r')\n",
    "\t# reading the text data\n",
    "\tdata = document.read()\n",
    "\t# closing the file\n",
    "\tdocument.close()\n",
    "\treturn data\n",
    " \n",
    "# turning the document into tokens after cleaning\n",
    "def clean_doc(file):\n",
    "\t# replacing '--' with spaces ' '\n",
    "\tfile = file.replace('--', ' ')\n",
    "\t# tokens are being created after splitting on spaces\n",
    "\ttokens = file.split()\n",
    "\t# removing the punctuations by creating a translation table\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# removing all the non alphabetic characters\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# converting all text into lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens\n",
    " \n",
    "# saving all the tokens in a file with one dialog being fed in one line\n",
    "def save_doc(lines, filename):\n",
    "  # forming the data line by line\n",
    "\tdata = '\\n'.join(lines)\n",
    "  # opening file in write mode.\n",
    "\tfile = open(filename, 'w')\n",
    "  # writing the data in the file\n",
    "\tfile.write(data)\n",
    "  # closing the file \n",
    "\tfile.close()\n",
    " \n",
    "# loading the document\n",
    "in_filename = \"/content/internet_archive_scifi_v3.txt\"\n",
    "doc = load_doc(in_filename)\n",
    "# printing the first 200 characters\n",
    "print(doc[:200])\n",
    " \n",
    "# cleaning the loaded document\n",
    "tokens = clean_doc(doc)\n",
    "# printing the cleaned document that is first 200 words\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    " \n",
    "# organize into sequences of tokens\n",
    "count = 0\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "  seq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "  line = ' '.join(seq)\n",
    "\t# storing them as a list of sequences\n",
    "  sequences.append(line)\n",
    "  count += 1\n",
    "\n",
    "  if count == 100000:\n",
    "    break\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    " \n",
    "# saving sequences to a file\n",
    "out_filename = 'sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzmrPuHKMrN8"
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmdSsqodMyZh"
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FO8PdAJHM0_s"
   },
   "outputs": [],
   "source": [
    "in_filename = 'sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "# creating lines from document \n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSI72Mg5M5Lp"
   },
   "outputs": [],
   "source": [
    "# encoding unique words in categories\n",
    "tokenizer = Tokenizer()\n",
    "# creating tokens of words in lines\n",
    "tokenizer.fit_on_texts(lines)\n",
    "# creating sequences as numeric array \n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size is the total unique words\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WabhiqDbM8FP"
   },
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "# converting sequences into array\n",
    "sequences = array(sequences)\n",
    "# taking all the previous words a input and last word as output\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "# converting Y to categorical which will give us the y as encoded value from vocab size\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofuLKCa8PYGL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'accuracy', min_delta = 0.001, \n",
    "                                              patience = 5, mode = 'max', verbose = 1,                                                        \n",
    "                                              restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCyJ0VbeM-t3",
    "outputId": "6a9ea396-bc67-49c6-d4e7-0009990eda59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            542800    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 250)           301000    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               194048    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10856)             705640    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,768,256\n",
      "Trainable params: 1,768,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "782/782 [==============================] - 62s 65ms/step - loss: 7.0964 - accuracy: 0.0572\n",
      "Epoch 2/200\n",
      "782/782 [==============================] - 21s 26ms/step - loss: 6.7158 - accuracy: 0.0601\n",
      "Epoch 3/200\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 6.4824 - accuracy: 0.0702\n",
      "Epoch 4/200\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 6.3051 - accuracy: 0.0812\n",
      "Epoch 5/200\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 6.1085 - accuracy: 0.0969\n",
      "Epoch 6/200\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 5.9569 - accuracy: 0.1031\n",
      "Epoch 7/200\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 5.8405 - accuracy: 0.1090\n",
      "Epoch 8/200\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 5.7362 - accuracy: 0.1139\n",
      "Epoch 9/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 5.6680 - accuracy: 0.1157\n",
      "Epoch 10/200\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 5.5611 - accuracy: 0.1209\n",
      "Epoch 11/200\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 5.4573 - accuracy: 0.1262\n",
      "Epoch 12/200\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 5.3686 - accuracy: 0.1302\n",
      "Epoch 13/200\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 5.2784 - accuracy: 0.1338\n",
      "Epoch 14/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 5.1884 - accuracy: 0.1385\n",
      "Epoch 15/200\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 5.0983 - accuracy: 0.1418\n",
      "Epoch 16/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 5.0102 - accuracy: 0.1454\n",
      "Epoch 17/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 4.9213 - accuracy: 0.1489\n",
      "Epoch 18/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 4.8340 - accuracy: 0.1525\n",
      "Epoch 19/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 4.7482 - accuracy: 0.1569\n",
      "Epoch 20/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 4.6657 - accuracy: 0.1609\n",
      "Epoch 21/200\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 4.5837 - accuracy: 0.1659\n",
      "Epoch 22/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 4.5047 - accuracy: 0.1710\n",
      "Epoch 23/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 4.4256 - accuracy: 0.1774\n",
      "Epoch 24/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 4.3493 - accuracy: 0.1830\n",
      "Epoch 25/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 4.2769 - accuracy: 0.1887\n",
      "Epoch 26/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 4.2057 - accuracy: 0.1948\n",
      "Epoch 27/200\n",
      "782/782 [==============================] - 14s 19ms/step - loss: 4.1355 - accuracy: 0.2012\n",
      "Epoch 28/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 4.0684 - accuracy: 0.2086\n",
      "Epoch 29/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 4.0043 - accuracy: 0.2148\n",
      "Epoch 30/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 3.9451 - accuracy: 0.2199\n",
      "Epoch 31/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 3.8809 - accuracy: 0.2281\n",
      "Epoch 32/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 3.8190 - accuracy: 0.2339\n",
      "Epoch 33/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 3.7550 - accuracy: 0.2418\n",
      "Epoch 34/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 3.6993 - accuracy: 0.2490\n",
      "Epoch 35/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 3.6388 - accuracy: 0.2560\n",
      "Epoch 36/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 3.5832 - accuracy: 0.2627\n",
      "Epoch 37/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 3.5259 - accuracy: 0.2719\n",
      "Epoch 38/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 3.4678 - accuracy: 0.2779\n",
      "Epoch 39/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 3.4220 - accuracy: 0.2857\n",
      "Epoch 40/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 3.3658 - accuracy: 0.2916\n",
      "Epoch 41/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 3.3080 - accuracy: 0.2998\n",
      "Epoch 42/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 3.2618 - accuracy: 0.3064\n",
      "Epoch 43/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 3.2120 - accuracy: 0.3138\n",
      "Epoch 44/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 3.1553 - accuracy: 0.3206\n",
      "Epoch 45/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 3.1050 - accuracy: 0.3288\n",
      "Epoch 46/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 3.0611 - accuracy: 0.3372\n",
      "Epoch 47/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 3.0171 - accuracy: 0.3434\n",
      "Epoch 48/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 2.9713 - accuracy: 0.3478\n",
      "Epoch 49/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 2.9153 - accuracy: 0.3567\n",
      "Epoch 50/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 2.8745 - accuracy: 0.3639\n",
      "Epoch 51/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 2.8347 - accuracy: 0.3706\n",
      "Epoch 52/200\n",
      "782/782 [==============================] - 14s 19ms/step - loss: 2.7906 - accuracy: 0.3767\n",
      "Epoch 53/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 2.7428 - accuracy: 0.3846\n",
      "Epoch 54/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 2.7006 - accuracy: 0.3913\n",
      "Epoch 55/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 2.6657 - accuracy: 0.3977\n",
      "Epoch 56/200\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 2.6131 - accuracy: 0.4058\n",
      "Epoch 57/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 2.5755 - accuracy: 0.4151\n",
      "Epoch 58/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 2.5340 - accuracy: 0.4207\n",
      "Epoch 59/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 2.4970 - accuracy: 0.4255\n",
      "Epoch 60/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 2.4616 - accuracy: 0.4324\n",
      "Epoch 61/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 2.4174 - accuracy: 0.4403\n",
      "Epoch 62/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 2.3896 - accuracy: 0.4468\n",
      "Epoch 63/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 2.3526 - accuracy: 0.4522\n",
      "Epoch 64/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 2.3164 - accuracy: 0.4580\n",
      "Epoch 65/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 2.2737 - accuracy: 0.4652\n",
      "Epoch 66/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 2.2317 - accuracy: 0.4741\n",
      "Epoch 67/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 2.2057 - accuracy: 0.4779\n",
      "Epoch 68/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 2.1722 - accuracy: 0.4846\n",
      "Epoch 69/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 2.1449 - accuracy: 0.4893\n",
      "Epoch 70/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 2.1020 - accuracy: 0.4978\n",
      "Epoch 71/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 2.0771 - accuracy: 0.5023\n",
      "Epoch 72/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 2.0499 - accuracy: 0.5056\n",
      "Epoch 73/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 2.0208 - accuracy: 0.5122\n",
      "Epoch 74/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 1.9759 - accuracy: 0.5226\n",
      "Epoch 75/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.9610 - accuracy: 0.5244\n",
      "Epoch 76/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.9353 - accuracy: 0.5294\n",
      "Epoch 77/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 1.8925 - accuracy: 0.5386\n",
      "Epoch 78/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 1.8627 - accuracy: 0.5443\n",
      "Epoch 79/200\n",
      "782/782 [==============================] - 14s 19ms/step - loss: 1.8610 - accuracy: 0.5459\n",
      "Epoch 80/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.8356 - accuracy: 0.5490\n",
      "Epoch 81/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.8002 - accuracy: 0.5572\n",
      "Epoch 82/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.7663 - accuracy: 0.5640\n",
      "Epoch 83/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.7541 - accuracy: 0.5671\n",
      "Epoch 84/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.7428 - accuracy: 0.5690\n",
      "Epoch 85/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 1.6799 - accuracy: 0.5838\n",
      "Epoch 86/200\n",
      "782/782 [==============================] - 14s 19ms/step - loss: 1.6596 - accuracy: 0.5860\n",
      "Epoch 87/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.6735 - accuracy: 0.5817\n",
      "Epoch 88/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.6407 - accuracy: 0.5885\n",
      "Epoch 89/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.6114 - accuracy: 0.5956\n",
      "Epoch 90/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.5871 - accuracy: 0.6015\n",
      "Epoch 91/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.5801 - accuracy: 0.6024\n",
      "Epoch 92/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.5469 - accuracy: 0.6105\n",
      "Epoch 93/200\n",
      "782/782 [==============================] - 14s 19ms/step - loss: 1.5144 - accuracy: 0.6167\n",
      "Epoch 94/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.5092 - accuracy: 0.6174\n",
      "Epoch 95/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.5022 - accuracy: 0.6175\n",
      "Epoch 96/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.4559 - accuracy: 0.6273\n",
      "Epoch 97/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.4412 - accuracy: 0.6323\n",
      "Epoch 98/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.4402 - accuracy: 0.6311\n",
      "Epoch 99/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.4108 - accuracy: 0.6388\n",
      "Epoch 100/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.3965 - accuracy: 0.6429\n",
      "Epoch 101/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.3921 - accuracy: 0.6423\n",
      "Epoch 102/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.3750 - accuracy: 0.6458\n",
      "Epoch 103/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.3479 - accuracy: 0.6518\n",
      "Epoch 104/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.3358 - accuracy: 0.6538\n",
      "Epoch 105/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.3301 - accuracy: 0.6577\n",
      "Epoch 106/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.3079 - accuracy: 0.6605\n",
      "Epoch 107/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.2865 - accuracy: 0.6657\n",
      "Epoch 108/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.2725 - accuracy: 0.6683\n",
      "Epoch 109/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 1.2473 - accuracy: 0.6747\n",
      "Epoch 110/200\n",
      "782/782 [==============================] - 14s 19ms/step - loss: 1.2361 - accuracy: 0.6771\n",
      "Epoch 111/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.2252 - accuracy: 0.6807\n",
      "Epoch 112/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.2159 - accuracy: 0.6810\n",
      "Epoch 113/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.1892 - accuracy: 0.6861\n",
      "Epoch 114/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.1778 - accuracy: 0.6899\n",
      "Epoch 115/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.1768 - accuracy: 0.6907\n",
      "Epoch 116/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.1731 - accuracy: 0.6904\n",
      "Epoch 117/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.1412 - accuracy: 0.6986\n",
      "Epoch 118/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 1.1322 - accuracy: 0.7011\n",
      "Epoch 119/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.1207 - accuracy: 0.7044\n",
      "Epoch 120/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.1198 - accuracy: 0.7038\n",
      "Epoch 121/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.1018 - accuracy: 0.7096\n",
      "Epoch 122/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.1013 - accuracy: 0.7087\n",
      "Epoch 123/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.0705 - accuracy: 0.7156\n",
      "Epoch 124/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 1.0563 - accuracy: 0.7184\n",
      "Epoch 125/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.0768 - accuracy: 0.7143\n",
      "Epoch 126/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.0403 - accuracy: 0.7222\n",
      "Epoch 127/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.0450 - accuracy: 0.7212\n",
      "Epoch 128/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.0235 - accuracy: 0.7272\n",
      "Epoch 129/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 1.0070 - accuracy: 0.7315\n",
      "Epoch 130/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 1.0012 - accuracy: 0.7329\n",
      "Epoch 131/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.9839 - accuracy: 0.7367\n",
      "Epoch 132/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.9967 - accuracy: 0.7324\n",
      "Epoch 133/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.9721 - accuracy: 0.7392\n",
      "Epoch 134/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.9626 - accuracy: 0.7417\n",
      "Epoch 135/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.9590 - accuracy: 0.7428\n",
      "Epoch 136/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.9430 - accuracy: 0.7474\n",
      "Epoch 137/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.9379 - accuracy: 0.7471\n",
      "Epoch 138/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.9517 - accuracy: 0.7436\n",
      "Epoch 139/200\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 0.9076 - accuracy: 0.7515\n",
      "Epoch 140/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.9138 - accuracy: 0.7529\n",
      "Epoch 141/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.9133 - accuracy: 0.7536\n",
      "Epoch 142/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.9216 - accuracy: 0.7496\n",
      "Epoch 143/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.8978 - accuracy: 0.7568\n",
      "Epoch 144/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.8718 - accuracy: 0.7638\n",
      "Epoch 145/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.8771 - accuracy: 0.7616\n",
      "Epoch 146/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.8734 - accuracy: 0.7633\n",
      "Epoch 147/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.8807 - accuracy: 0.7624\n",
      "Epoch 148/200\n",
      "782/782 [==============================] - 14s 19ms/step - loss: 0.8645 - accuracy: 0.7659\n",
      "Epoch 149/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.8336 - accuracy: 0.7739\n",
      "Epoch 150/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.8483 - accuracy: 0.7684\n",
      "Epoch 151/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.8596 - accuracy: 0.7663\n",
      "Epoch 152/200\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.8268 - accuracy: 0.7742\n",
      "Epoch 153/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.8193 - accuracy: 0.7765\n",
      "Epoch 154/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.8035 - accuracy: 0.7794\n",
      "Epoch 155/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.8260 - accuracy: 0.7725\n",
      "Epoch 156/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7960 - accuracy: 0.7834\n",
      "Epoch 157/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7943 - accuracy: 0.7824\n",
      "Epoch 158/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.8162 - accuracy: 0.7766\n",
      "Epoch 159/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7744 - accuracy: 0.7866\n",
      "Epoch 160/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7943 - accuracy: 0.7825\n",
      "Epoch 161/200\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 0.7849 - accuracy: 0.7854\n",
      "Epoch 162/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7773 - accuracy: 0.7866\n",
      "Epoch 163/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7418 - accuracy: 0.7944\n",
      "Epoch 164/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7605 - accuracy: 0.7903\n",
      "Epoch 165/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7619 - accuracy: 0.7904\n",
      "Epoch 166/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7535 - accuracy: 0.7917\n",
      "Epoch 167/200\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.7590 - accuracy: 0.7908\n",
      "Epoch 168/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7422 - accuracy: 0.7961\n",
      "Epoch 169/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7467 - accuracy: 0.7946\n",
      "Epoch 170/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7147 - accuracy: 0.8033\n",
      "Epoch 171/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7351 - accuracy: 0.7986\n",
      "Epoch 172/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7284 - accuracy: 0.7970\n",
      "Epoch 173/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6999 - accuracy: 0.8069\n",
      "Epoch 174/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7114 - accuracy: 0.8036\n",
      "Epoch 175/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7055 - accuracy: 0.8049\n",
      "Epoch 176/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6942 - accuracy: 0.8081\n",
      "Epoch 177/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7141 - accuracy: 0.8019\n",
      "Epoch 178/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6904 - accuracy: 0.8094\n",
      "Epoch 179/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7017 - accuracy: 0.8053\n",
      "Epoch 180/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6788 - accuracy: 0.8114\n",
      "Epoch 181/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6519 - accuracy: 0.8189\n",
      "Epoch 182/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6815 - accuracy: 0.8099\n",
      "Epoch 183/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6951 - accuracy: 0.8072\n",
      "Epoch 184/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6778 - accuracy: 0.8116\n",
      "Epoch 185/200\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 0.6528 - accuracy: 0.8184\n",
      "Epoch 186/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6697 - accuracy: 0.8156\n",
      "Epoch 187/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6648 - accuracy: 0.8159\n",
      "Epoch 188/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6738 - accuracy: 0.8141\n",
      "Epoch 189/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6255 - accuracy: 0.8250\n",
      "Epoch 190/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6259 - accuracy: 0.8245\n",
      "Epoch 191/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6640 - accuracy: 0.8151\n",
      "Epoch 192/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6390 - accuracy: 0.8230\n",
      "Epoch 193/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6219 - accuracy: 0.8267\n",
      "Epoch 194/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6658 - accuracy: 0.8153\n",
      "Epoch 195/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6386 - accuracy: 0.8219\n",
      "Epoch 196/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.5948 - accuracy: 0.8332\n",
      "Epoch 197/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6035 - accuracy: 0.8319\n",
      "Epoch 198/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6518 - accuracy: 0.8198\n",
      "Epoch 199/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6401 - accuracy: 0.8224\n",
      "Epoch 200/200\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.5885 - accuracy: 0.8359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f10ba56a1f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining the  model\n",
    "model = Sequential()\n",
    "# embedding layer to take the whole vocab as input and give output of sequence length and total words that is 50\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "# adding LSTM layer return sequences has to be true to add another LSTM layer\n",
    "model.add(LSTM(250, return_sequences=True))\n",
    "# adding another LSTM layer\n",
    "model.add(LSTM(128))\n",
    "# adding a hidden layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# adding a hidden layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# output layer with whole vocabulary\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compiling model with categorical crossentopy loss and adam optimiser\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fitting the model\n",
    "model.fit(X, y, batch_size=128, epochs=200)\n",
    " \n",
    "# saving the model as a h5 file\n",
    "# model.save('model.h5')\n",
    "# saving the tokenizer as a pickle file\n",
    "# dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wPJUXbzVADl"
   },
   "outputs": [],
   "source": [
    "# saving the model as a h5 file\n",
    "model.save('model.h5')\n",
    "# saving the tokenizer as a pickle file\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDPxL4SNNPLT"
   },
   "outputs": [],
   "source": [
    "# to generate randon integers\n",
    "from random import randint\n",
    "# to load the pickle file\n",
    "from pickle import load\n",
    "# loading the model from h5\n",
    "from keras.models import load_model\n",
    "# to provide padding for short sentences\n",
    "from keras.utils import pad_sequences\n",
    "# to process the array\n",
    "import numpy as np\n",
    "\n",
    "# opening the file and returning it as text\n",
    "def load_doc(filename):\n",
    "  # opening the file in read only mode\n",
    "  file= open(filename,'r')\n",
    "  # extracting the text from the file\n",
    "  text=file.read()\n",
    "  # closing the file\n",
    "  file.close()\n",
    "  # returning the text\n",
    "  return text\n",
    "\n",
    "# function to generate sentences\n",
    "def generate_seq(model, tokenizer,seq_length,seed_text,n_words):\n",
    "  # creating a list for result\n",
    "  result= list()\n",
    "  # taking the input text as seed text\n",
    "  in_text=seed_text\n",
    "  # for the length of the number of words to be genrated \n",
    "  for _ in range(n_words):\n",
    "    # encoding the words as sequences\n",
    "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    # padding the rest of the words\n",
    "    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "    # predicting the numpy argumant for the input \n",
    "    yhat = np.argmax(model.predict(encoded), axis=-1)\n",
    "    # initialising the output sequence\n",
    "    out_word = ''\n",
    "    # converting category back to word \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "      if index == yhat:\n",
    "        out_word = word\n",
    "        break\n",
    "    # appending the text that is generated\n",
    "    in_text += ' ' + out_word\n",
    "    # forming the result\n",
    "    result.append(out_word)\n",
    "  # returning the result\n",
    "  return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ox35wqNwUrME",
    "outputId": "9b5f55ce-d60c-43fb-b431-8583fb4faa44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not know whether Papa guessed my feelings on the subject, probably not, and in any case he would not have been interested. The opinion of other people never interested him in the slightest degree. I think it was really a sign of his greatness. In the same way, he lived quite detached from the necessities of daily life. He ate what was put before him in an exemplary fashion, but seemed mildly pained when the question of paying\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "generated text: offensively nine days in the permanent clip are listening right without one its in any times he said who first little friend and can make them deep please an anthrovac droned in not more than phenomenal he sank out of the ship but now more in a week then you\n"
     ]
    }
   ],
   "source": [
    "# providing the input file\n",
    "in_filename = 'sequences.txt'\n",
    "# loading the document\n",
    "doc = load_doc(in_filename)\n",
    "#splitting it into lines\n",
    "lines = doc.split('\\n')\n",
    "# creating the sequence length\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "#loading the predictive model\n",
    "model = load_model('model.h5')\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "#seed_text = lines[randint(0,len(lines))]\n",
    "#if you want to provide your own input just uncomment the below line and put your own input inside the ''.\n",
    "seed_text = 'I do not know whether Papa guessed my feelings on the subject, probably not, and in any case he would not have been interested. The opinion of other people never interested him in the slightest degree. I think it was really a sign of his greatness. In the same way, he lived quite detached from the necessities of daily life. He ate what was put before him in an exemplary fashion, but seemed mildly pained when the question of paying'\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(\"generated text:\",generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEbZ6WN4U5r_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
